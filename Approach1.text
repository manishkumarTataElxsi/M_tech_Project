You're aiming to automate the generation of Gherkin test scripts i.e. Behavior Driven Development(BDD) from an ADAS (Advanced Driver-Assistance Systems) user manual using a multi-agent system. Here's a breakdown of how you could approach each step and integrate them into a cohesive system:

Overall Architecture:

A multi-agent system is a good choice. You'll have specialized agents for each task, communicating and coordinating to achieve the overall goal. A central "Orchestrator" agent can manage the workflow and data flow between the other agents.

Agents and their Responsibilities:

Requirement Extraction Agent:
Input: ADAS User Manual (PDF, Word, or other format).
Process: Uses Natural Language Processing (NLP) techniques like Named Entity Recognition (NER), dependency parsing, and semantic analysis to identify and extract functional requirements. This agent needs to understand the structure and language of technical documentation.
Output: Structured requirements (e.g., in JSON or a database). Each requirement should have a unique ID, a textual description, and potentially associated metadata (e.g., priority, category).

Gherkin Generation Agent:
Input: Structured requirements.
Process: Translates the requirements into Gherkin scenarios. This agent needs to understand Gherkin syntax (Given, When, Then, And, But) and how to map requirements to executable steps. It may use templates or rules-based translation.
Output: Gherkin feature files.

Missing Test Case Analysis Agent:
Input: Structured requirements and generated Gherkin scenarios.
Process: Compares the generated Gherkin scenarios against the original requirements to identify gaps. It looks for requirements that haven't been adequately covered by test scenarios. This agent needs to understand the semantic relationship between requirements and test cases.
Output: A report of missing test cases, specifying which requirements are not covered or are only partially covered.

Traceability Report Generation Agent:
Input: Structured requirements, generated Gherkin scenarios, and missing test case report.
Process: Creates a traceability matrix or report that links requirements to Gherkin scenarios and highlights missing test cases. This report can be in various formats (e.g., CSV, HTML, or a dedicated reporting tool).
Output: Traceability report.

Orchestrator Agent:
Responsibility: Manages the workflow, coordinates the other agents, stores and retrieves data, and controls the iterative process.
Step-by-Step Implementation:

Requirement Extraction: Focus on robust NLP techniques. Consider using pre-trained language models fine-tuned on technical text.
Gherkin Generation: Start with simple rule-based translation and gradually introduce more sophisticated techniques like template-based generation or even machine learning models trained on examples of requirements and their corresponding Gherkin scenarios.
Missing Test Case Analysis: This is crucial. Define clear criteria for what constitutes adequate coverage. You might use techniques like keyword matching, semantic similarity, or even more advanced methods like formal verification to ensure complete coverage.
Traceability Report: Use a format that allows easy analysis and tracking. Consider using a dedicated test management tool for better traceability and reporting capabilities.
Iteration: The Orchestrator agent controls the loop. It checks the traceability report. If missing test cases are identified, it triggers the Gherkin Generation Agent (potentially with refined input or constraints) to generate additional scenarios. This process repeats until the traceability report shows sufficient coverage.
Tools and Technologies:

Programming Languages: Python (with libraries like NLTK, spaCy, transformers) is a good choice for NLP and agent development.
LLMs : Gemini
Agent Frameworks: Consider using an agent framework like CrewAI.
Data Storage: A database (e.g., PostgreSQL) or a document store (e.g., MongoDB) can be used to store requirements, Gherkin scenarios, and reports.
Challenges:

Ambiguity in User Manuals: Natural language can be ambiguous. The Requirement Extraction Agent needs to be robust enough to handle this.
Complex Requirements: Some requirements might be complex and difficult to translate directly into Gherkin scenarios.
Defining Coverage: Determining what constitutes "sufficient coverage" can be challenging.
By following this approach, you can create a powerful automated system for generating Gherkin test scripts from ADAS user manuals, improving test coverage and reducing manual effort. Remember to start with a small, well-defined subset of the manual and gradually expand the system's capabilities.


Executing a project of this complexity requires a phased approach. Here's a breakdown of how to get started and iteratively build the system:

Phase 1: Proof of Concept (POC)

Select a Small Scope: Don't try to process the entire ADAS manual at once. Choose a small, well-defined section with a few key features. This will allow you to focus on core functionality and iterate quickly.

Prototype Core Agents:

Requirement Extraction Agent (Simplified): Start with basic NLP techniques like regular expressions or keyword extraction to identify simple requirements.

Gherkin Generation Agent (Rule-Based): Create a set of simple rules to translate the extracted requirements into basic Gherkin scenarios. For example:
Requirement: "The system shall warn the driver if the vehicle deviates from its lane."
Rule: "If requirement contains 'warn the driver' and 'deviates from its lane', generate 'Given the vehicle is driving on a marked lane', 'When the vehicle deviates from the lane', 'Then the system should issue a warning'."

Missing Test Case Analysis Agent: Identify the missing test cases using requirements and Gherkin test script

Orchestrator Agent (Basic): Implement a simple script to run the other two agents sequentially and print the generated Gherkin.
Test and Iterate: Run the POC on your chosen section of the manual. Analyze the results, identify weaknesses in the extraction and generation processes, and refine your agents.

Phase 2: Expanding Functionality

Improve Requirement Extraction:

Advanced NLP: Implement more sophisticated NLP techniques like Named Entity Recognition (NER), dependency parsing, and semantic analysis using libraries like spaCy or transformers.
Handle Complex Sentences: Address issues like complex sentence structures, negations, and conditional statements.
Requirement Structuring: Define a clear structure for storing requirements (e.g., using JSON or a database) with fields like ID, description, priority, category, etc.
Enhance Gherkin Generation:

Template-Based Generation: Use templates to generate more varied and realistic Gherkin scenarios.
Parameterization: Introduce parameters in Gherkin scenarios to handle different input values and test cases.
Context and Background: Implement the use of Background steps in Gherkin to define common preconditions.
Implement Missing Test Case Analysis:

Coverage Criteria: Define clear criteria for determining adequate test coverage.
Comparison Logic: Implement logic to compare requirements and generated scenarios, identifying missing or partially covered requirements.
Implement Traceability Report Generation:

Report Format: Choose a suitable report format (e.g., CSV, HTML) or use a dedicated test management tool.
Linking Requirements and Scenarios: Implement logic to link requirements to corresponding Gherkin scenarios in the report.
Phase 3: Advanced Features and Refinement

Advanced NLP for Requirement Understanding:

Semantic Similarity: Use techniques like word embeddings or sentence embeddings to measure the semantic similarity between requirements and Gherkin scenarios.
Coreference Resolution: Implement coreference resolution to handle pronouns and other references in the text.
Machine Learning for Gherkin Generation:

Train a Model: Train a machine learning model on a dataset of requirements and their corresponding Gherkin scenarios to improve the accuracy and naturalness of the generated tests.
Agent Communication and Coordination:

Agent Framework: Use an agent framework like JADE or an alternative in Python to manage agent communication and coordination more effectively.
Integration with Testing Tools:

Automated Test Execution: Integrate the generated Gherkin scenarios with a testing framework like Cucumber or Behave to enable automated test execution.
Tools and Technologies (Expanded):

Programming Language: Python (with libraries like NLTK, spaCy, transformers, scikit-learn).
Agent Frameworks: CrewAI
Gherkin Tools: Behave (Python).
NLP Libraries: spaCy, NLTK, transformers (Hugging Face),Gemini.
Machine Learning Libraries: scikit-learn, TensorFlow, PyTorch.
Data Storage: PostgreSQL, MongoDB.
Test Management Tools: Jira, TestRail, Xray.
Key Considerations:

Data: You'll need a good dataset of ADAS user manual text and ideally some examples of existing Gherkin test cases to train any machine learning models.
Evaluation: Define clear metrics to evaluate the performance of your system (e.g., precision, recall, F1-score for requirement extraction, coverage rate for test generation).
Iteration: This is an iterative process. Start small, test frequently, and refine your system based on the results.
This structured approach will help you break down this complex project into manageable steps and increase your chances of success. Remember to document your progress and findings along the way.





# Phase 1: Proof of Concept (Simplified)

import re

class RequirementExtractionAgent:
    def extract_requirements(self, text):
        # Very basic extraction using regular expressions
        requirements = re.findall(r"The system shall (.*?)\.", text)
        return requirements

class GherkinGenerationAgent:
    def generate_gherkin(self, requirements):
        gherkin = []
        for req in requirements:
            if "warn the driver" in req and "deviates from its lane" in req:
                gherkin.append("Scenario: Lane Departure Warning")
                gherkin.append("  Given the vehicle is driving on a marked lane")
                gherkin.append("  When the vehicle deviates from the lane")
                gherkin.append("  Then the system should issue a warning")
            elif "apply brakes" in req and "imminent collision" in req:
                gherkin.append("Scenario: Automatic Emergency Braking")
                gherkin.append("  Given an imminent collision is detected")
                gherkin.append("  When the vehicle is approaching an obstacle too quickly")
                gherkin.append("  Then the system should apply the brakes")
            else:
              gherkin.append("Scenario: Requirement not covered by rules")
              gherkin.append(f"  # Requirement: The system shall {req}.")

        return "\n".join(gherkin)

# Example ADAS manual text (very simplified)
adas_text = """
The system shall warn the driver if the vehicle deviates from its lane.
The system shall apply brakes if an imminent collision is detected.
The system shall maintain a safe following distance.
"""

# Orchestrator
extractor = RequirementExtractionAgent()
generator = GherkinGenerationAgent()

requirements = extractor.extract_requirements(adas_text)
gherkin_script = generator.generate_gherkin(requirements)

print(gherkin_script)


# --- Phase 2 additions (Illustrative) ---
import json

class StructuredRequirement:
    def __init__(self, id, description, priority="Medium", category="Safety"):
        self.id = id
        self.description = description
        self.priority = priority
        self.category = category

    def to_json(self):
        return json.dumps(self.__dict__)

class AdvancedRequirementExtractionAgent:
    def extract_structured_requirements(self, text):
        requirements = []
        extracted = re.findall(r"The system shall (.*?)\.", text)
        for i, desc in enumerate(extracted):
            req = StructuredRequirement(id=f"REQ-{i+1}", description=desc)
            requirements.append(req)
        return requirements

#Example usage of structured requirements
advanced_extractor = AdvancedRequirementExtractionAgent()
structured_requirements = advanced_extractor.extract_structured_requirements(adas_text)
for req in structured_requirements:
    print(req.to_json())
Explanation and Improvements:

Phase 1 (Basic):
RequirementExtractionAgent: Now uses regular expressions to find sentences starting with "The system shall". This is still very basic but better than just splitting the text.
GherkinGenerationAgent: Implemented simple rules to translate specific requirements into Gherkin scenarios. Added a default scenario for requirements not covered by the rules.
Orchestrator: Creates instances of the agents, runs them, and prints the generated Gherkin.
Phase 2 (Illustrative):
StructuredRequirement: A class to represent requirements with structured data (ID, description, priority, category). This is a crucial step towards better data management.
AdvancedRequirementExtractionAgent: Creates structured requirements from the extracted text.
How to Run:

Save the code as a Python file (e.g., adas_test_automation.py).
Run it from your terminal: python adas_test_automation.py
Next Steps and Key Improvements for Further Development:

NLP for Requirement Extraction: Replace the regular expressions with spaCy or transformers for more accurate and robust extraction. This will handle more complex sentence structures and semantic understanding.
Template-Based Gherkin Generation: Use templating engines (like Jinja2 in Python) to create more flexible and reusable Gherkin scenarios.
Missing Test Case Analysis: Implement logic to compare the generated Gherkin against the structured requirements to identify gaps.
Traceability Report: Generate a report (e.g., CSV or JSON) linking requirements to Gherkin scenarios and highlighting missing test cases.
Agent Framework: Consider using an agent framework for better agent communication and coordination.
Error Handling: Add error handling to make the code more robust.
Testing: Write unit tests for your agents to ensure they are working correctly.
This improved example provides a more solid foundation to build upon. Remember to iterate and gradually add more advanced features as needed. This simplified example is a starting point, and implementing a full multi-agent system with advanced NLP and machine learning would require significantly more code and effort.






I want to Automate the Gherkin test script Generation the ADAS user manual using multi-agent system and Generative AI:

Step1: requirements generation from ADAS user manual

Step2: Gherkin test script generation



Automating Gherkin test script generation from an ADAS user manual using a multi-agent system and generative AI involves combining the strengths of both approaches. Here's a refined approach focusing on integrating generative AI:

Architecture:

We'll still use a multi-agent system for organization and workflow, but we'll leverage generative AI models (like large language models - LLMs) for the more complex tasks of requirement understanding and Gherkin generation.

Agents and their Responsibilities:

Document Processing Agent:

Input: ADAS User Manual (PDF, Word, etc.).
Process: Converts the manual into plain text. Handles document formatting and structure.
Output: Cleaned text content.
Requirement Extraction Agent (Enhanced with LLM):

Input: Cleaned text content.
Process: Uses an LLM (e.g., GPT-3.5 Turbo, Gemini) with careful prompting to:
Identify functional requirements.
Classify requirements (e.g., safety, performance, usability).
Extract key information related to each requirement (e.g., inputs, outputs, conditions).
Prompt Engineering is Key: You'll need to craft effective prompts to guide the LLM. For example:
"Extract the functional requirements from the following ADAS user manual text and present them in a structured format (JSON). Include the requirement ID, a concise description, the type of requirement (e.g., safety, performance), and any relevant inputs, outputs, and conditions.

Text: [Paste the ADAS manual text here]"
Output: Structured requirements (e.g., JSON).
Gherkin Generation Agent (LLM-Powered):

Input: Structured requirements.
Process: Uses an LLM with carefully designed prompts to generate Gherkin scenarios for each requirement.
Prompt Example:
"Generate Gherkin test scenarios for the following ADAS requirement:

Requirement: {Requirement JSON from previous step}

Focus on creating scenarios that cover positive and negative cases, boundary conditions, and error handling. Ensure the Gherkin is well-structured and follows best practices."
Output: Gherkin feature files.
Validation Agent (Optional):

Input: Generated Gherkin.
Process: Can use simpler rule-based checks or even another LLM to validate the generated Gherkin for syntax, completeness, and coverage.
Output: Validation report.
Orchestrator Agent: Manages workflow, data flow, and agent coordination.

Workflow:

The Document Processing Agent cleans the ADAS manual text.
The Orchestrator sends the cleaned text to the Requirement Extraction Agent.
The Requirement Extraction Agent uses the LLM with a well-crafted prompt to extract structured requirements.
The Orchestrator sends the structured requirements to the Gherkin Generation Agent.
The Gherkin Generation Agent uses the LLM with a specific prompt to generate Gherkin scenarios.
(Optional) The Validation Agent checks the generated Gherkin.
The Orchestrator collects the results and generates reports.
Key Improvements with Generative AI:

Improved Requirement Understanding: LLMs can better understand the nuances of natural language, leading to more accurate and complete requirement extraction.
More Natural Gherkin: LLMs can generate more human-like and readable Gherkin scenarios.
Handling Complexity: LLMs can handle more complex requirements and generate more varied test cases.
Reduced Manual Effort: Significantly reduces the need for manual rule creation and template design.
Implementation Details:

LLM API: Use an API like OpenAI's API (for GPT models) or Google's Vertex AI (for Gemini models).
Prompt Engineering: This is crucial. Experiment with different prompts to get the best results. Iterate and refine your prompts based on the output.
Data Storage: Use a database or JSON files to store the structured requirements and Gherkin scenarios.
Agent Framework (Optional but Recommended): Using an agent framework can help manage the complexity of multiple interacting agents.
Example (Conceptual Python with LLM API Call):

Python

import openai  # Or google.generativeai for Gemini

# ... (Document Processing Agent) ...

class RequirementExtractionAgent:
    def extract_requirements(self, text):
        response = openai.ChatCompletion.create( #Or Gemini API call
            model="gpt-3.5-turbo", #Or Gemini model
            messages=[
                {"role": "system", "content": "You are an expert in extracting functional requirements from technical documents."},
                {"role": "user", "content": f"Extract requirements:\n\n{text}"} #Improved prompt
            ]
        )
        requirements_json = response['choices'][0]['message']['content'] #Extract the JSON
        return json.loads(requirements_json) #Parse the JSON

# ... (Gherkin Generation Agent - similar structure with different prompt) ...
Challenges:

LLM Cost: Using LLM APIs can be expensive, especially for large documents.
Prompt Engineering: Requires careful design and experimentation.
Output Validation: While LLMs are good at generating text, they can still make mistakes. Validation is important.
Context Window Limits: LLMs have context window limits. For very large documents, you might need to process them in chunks.
By combining a multi-agent system with the power of generative AI, you can create a highly effective automated system for Gherkin test script generation, significantly reducing manual effort and improving test quality. Remember to focus on prompt engineering and validation to achieve the best results.






execute this



Python

import openai  # Or google.generativeai for Gemini
import json
import os

# Set your OpenAI API key (or Gemini API key)
openai.api_key = os.getenv("OPENAI_API_KEY") #Get API key from environment variable

# Ensure you have the openai package: pip install openai

class DocumentProcessingAgent:
    def process_document(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as file:  # Handle various encodings
                text = file.read()
            return text
        except FileNotFoundError:
            return "Error: File not found."
        except Exception as e:
            return f"Error processing document: {e}"

class RequirementExtractionAgent:
    def extract_requirements(self, text):
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",  # Or a more capable model like gpt-4
                messages=[
                    {"role": "system", "content": """You are an expert in extracting functional requirements from technical documents. Extract the functional requirements from the following ADAS user manual text and present them in a structured JSON format. Include the requirement ID (REQ-1, REQ-2, etc.), a concise description, the type of requirement (e.g., safety, performance, usability), and any relevant inputs, outputs, and conditions. If a field is not applicable, use "N/A"."""},
                    {"role": "user", "content": f"Extract requirements:\n\n{text}"}
                ],
                temperature=0.0 #Lower temperature for more deterministic output
            )
            requirements_json = response['choices'][0]['message']['content']
            try:
                requirements = json.loads(requirements_json)
                return requirements
            except json.JSONDecodeError as e:
                print(f"JSON Decode Error:{e}")
                print(f"Raw LLM Output:\n{requirements_json}") #Print the raw output for debugging
                return None #Return None if JSON decoding fails
        except openai.error.OpenAIError as e:
            print(f"OpenAI API Error: {e}")
            return None

class GherkinGenerationAgent:
    def generate_gherkin(self, requirements):
        gherkin_scenarios = []
        if requirements:
            for req in requirements:
                try:
                    response = openai.ChatCompletion.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "You are an expert in generating Gherkin test scenarios from requirements. Generate Gherkin test scenarios for the provided ADAS requirement. Focus on creating scenarios that cover positive and negative cases, boundary conditions, and error handling. Ensure the Gherkin is well-structured and follows best practices."},
                            {"role": "user", "content": f"Generate Gherkin:\n\n{json.dumps(req)}"}
                        ],
                        temperature=0.0
                    )
                    gherkin_text = response['choices'][0]['message']['content']
                    gherkin_scenarios.append(gherkin_text)
                except openai.error.OpenAIError as e:
                    print(f"OpenAI API Error: {e}")
                    return None
        return "\n\n".join(gherkin_scenarios)

# Example usage (Orchestrator):
if __name__ == "__main__":
    doc_processor = DocumentProcessingAgent()
    req_extractor = RequirementExtractionAgent()
    gherkin_generator = GherkinGenerationAgent()

    file_path = "adas_manual.txt"  # Replace with your ADAS manual file
    processed_text = doc_processor.process_document(file_path)

    if "Error" not in processed_text: #Check for processing error
        requirements = req_extractor.extract_requirements(processed_text)
        if requirements:
            print("Extracted Requirements (JSON):")
            print(json.dumps(requirements, indent=4)) #Print formatted JSON
            gherkin = gherkin_generator.generate_gherkin(requirements)
            if gherkin:
                print("\nGenerated Gherkin:")
                print(gherkin)
            else:
                print("Gherkin generation failed.")
        else:
            print("Requirement extraction failed.")
    else:
        print(processed_text) #Print the error message

Key Improvements:

Error Handling: Added try...except blocks to handle file processing errors, JSON decoding errors, and OpenAI API errors. This makes the code much more robust.
Clearer Prompts: Improved the prompts to be more specific and provide better guidance to the LLM.
JSON Handling: Explicitly handles JSON parsing and includes error handling for invalid JSON responses from the LLM. Prints the raw LLM output on JSON errors for debugging.
Temperature Control: Set temperature=0.0 in the LLM calls for more deterministic output, which is generally better for test case generation.
File Handling: Reads the ADAS manual from a file, making it more practical. Added UTF-8 encoding to handle more character sets.
Main Block: Wrapped the main execution in if __name__ == "__main__": so the code is only executed when the script is run directly.
Formatted JSON Output: Prints the extracted requirements as formatted JSON for better readability.
Check for Processing Errors: The orchestrator checks for errors during document processing before proceeding.
API Key from Environment: Gets the OpenAI API key from the environment variable OPENAI_API_KEY. This is best practice for security.
To Run:

Install the OpenAI Python library: pip install openai
Set your OpenAI API key:
The best way is to set it as an environment variable:
On Linux/macOS: export OPENAI_API_KEY="your_actual_api_key"
On Windows: set OPENAI_API_KEY="your_actual_api_key"
Or, you can replace os.getenv("OPENAI_API_KEY") with your actual key in the code (but this is less secure).
Create an adas_manual.txt file: Put some sample ADAS manual text into this file. For example:
The system shall warn the driver if the vehicle deviates from its lane. The system shall apply the brakes if an imminent collision is detected. The system shall maintain a safe following distance.
Run the script: python your_script_name.py
This improved version is much more complete and robust. It addresses many of the potential issues and provides a more practical starting point for building your automated test generation system. Remember that prompt engineering is key to getting good results from the LLM. Experiment with different prompts to find what works best for your specific ADAS manual.



```


GEMINI_API_KEY= "AIzaSyC0oJrWCInA6ng2yaOIkEU7xYk8do2udag"#"AIzaSyCxkO3jDlUKKyJ_oWlkk32W7AUQuPoN48k"#"AIzaSyDOSHsc0DwLYmHin-x2f8YJdityflEjM_I"
os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY

# Advanced configuration with detailed parameters
llm = LLM(
    model="gemini/gemini-pro",
    temperature=0.3,        # Higher for more creative outputs
    max_tokens=32000,       # Maximum length of response
)


requirement_agent = Agent(
    role="Requirements_generator",
    goal="Extract comprehensive system requirements.",
    backstory="""You are expert in reading and understanding technical documantation and can extract testable requirements from Software Requirement Specifications(SRS) file.
                You are highly skilled and experienced requirements engineer, specializing in crafting precise and exhaustive requirements for software systems.
                You have a deep understanding of  Software Requirement Specifications(SRS) and can extract essential information to create a comprehensive set of requirements for testing.
                You will carefully consider all relevant aspects of the  Software Requirement Specifications(SRS) to extract a comprehensive set of requirements.
                """,
    #tools = [tool],
    allow_delegation=False,
    max_iter =100,
    verbose=True,
    llm=llm
)

# Creating Feature Generation Agent
feature_generation_agent = Agent(
    role="Feature_generation_expert",
    goal="Generate features for Behavior-Driven Development based on the provided system requirements.",
    backstory="""You are an expert in analyzing system requirements and identifying distinct features described within them.
                Your goal is to extract and organize features for Behavior-Driven Developmen that are described in the requirements in a clear and structured manner.""",
    allow_delegation=False,
    max_iter=100,
    verbose=True,
    llm=llm
)

# Creating Scenario Generation Agent
scenario_generation_agent = Agent(
    role="Scenario_generation_expert",
    goal="Generate all possible scenarios for each identified feature.",
    backstory="""You are an expert in scenario analysis and generation.
                 Given a list of features, your goal is to generate all potential scenarios for each feature,
                 including edge cases and error handling scenarios, that are relevant for each feature.""",
    allow_delegation=False,
    max_iter=100,
    verbose=True,
    llm=llm
)

# Creating an agent to generate test cases
testcase_agent = Agent(
    role="Test case generator",
    goal="""Generate the Gherkin test cases for Behavior-Driven Developmen based on the extracted features , Scenarios from requirements""",
    backstory="""You are a highly skilled and experienced test case generation expert,
                 You possess a deep understanding of software testing methodologies and have a keen eye for identifying potential defects,
                 Given a clear and concise description of a system or feature,
                 You can generate a robust set of test cases that cover a wide range of scenarios, including positive, negative, boundary, and edge cases.
                 You am proficient in various testing techniques such as functional testing, integration testing, and system testing.
                 You goal is to help ensure the quality and reliability of software applications by providing thorough and effective test cases.
                 """,
    allow_delegation=False,
    max_iter =100,
    verbose=True,
    llm=llm
)

# Creating a quality assurance expert which identifies missing test cases
quality_assurance_agent = Agent(
    role="Quality assurance expert",
    goal="""Identifying missing test cases in test coverage.
            Review the original given text and the generated test cases.
            And generate the missing test cases.""",
    backstory="""You are a seasoned quality assurance expert.
                 You have a deep understanding of industry regulations and use your analytical skills to design comprehensive test cases that uncover hidden bugs.
                 You work closely with developers and testers to ensure that our software meets the highest quality standards.
                  """,
    allow_delegation=False,
    max_iter =100,
    verbose=True,
    llm=llm
)

# Creating an agent to convert features into Python scripts using Behave
behave_script_agent = Agent(
    role="Behave Script Generator",
    goal="Generate Python scripts for Behavior-Driven Development using the Behave library based on the provided features and scenarios.",
    backstory="""You are an expert in Behavior-Driven Development and proficient in generating Python scripts for the Behave library.
                 Your task is to convert each feature and its associated scenarios into a properly structured Behave feature file and corresponding Python step definitions.
                 You have extensive experience in Python and the Behave library, ensuring that the generated scripts are syntactically correct and follow best practices.
                 Your output should include both the feature file and the corresponding step definition scripts. Ensure coverage of all scenarios and include proper imports and comments.
                 NOTE:Provide a complete output.
                 """,
    allow_delegation=False,
    max_iter=100,
    verbose=True,
    llm=llm
)


# Creating the task for requirement generation which is done by the requirement_generator agent
requirements = Task(
    description=f"""Following these guidelines, generate system requirements using the provided Software Requirement Specifications(SRS) {full_pdf_content} in a clear and concise manner:

                    1. Identify each functionality and scenario described.
                    2. For each identified functionality, create a requirement with a unique identifier (e.g., REQ-001, REQ-002...).
                    3. Ensure requirements are phrased as clear and unambiguous statements using complete sentences.
                    6. Organize the requirements in an Excel file with columns: Requirement ID | Requirement Description

                    NOTE: Please generate the requirements based on the provided Software Requirement Specifications(SRS) only.
                    **IMPORTANT**: Ensure no requirement is left out. If necessary, continue generating additional files to include all requirements
                    """,
    expected_output="""Format your response as if you were describing the contents of an Excel file:
                        Requirement ID | Requirement Description

                        REQ-001 | <requirement1>
                        REQ-002 | <requirement2>
                           .
                           .
                           .

                        IMPORTANT: Your response should be a valid Excel file (.xlsx).
                                   Your response must consist only of the table structure.
                                   Do not add any additional text, explanations, or summary lines before or after the table.
                                   The response should start with the header row and end with the last data row of the table.
                    """,
    output_file = "requirements6.csv",
    agent=requirement_agent
)


# Creating Task for Feature Generation from Requirements
feature_generation_task = Task(
    description=f"""Analyze the provided system requirements  using the provided Software Requirement Specifications(SRS)  and generate a list of distinct features. Each feature should be clearly defined, unique, and relevant to the system being described.

                   """,
    expected_output="""Format your response as if you were describing the contents of an Excel file:
                        Feature ID | Feature Description | Requirement ID

                        FTE-001 | <feature1> | <requirement_id>
                        FTE-002 | <feature2> | <requirement_id>
                           .
                           .
                           .
                        IMPORTANT: Your response should be a valid Excel file (.xlsx).
                                  Your response must consist only of the table structure.
                                  Do not add any additional text, explanations, or summary lines before or after the table.
                                  The response should start with the header row and end with the last data row of the table.
                                  Ensure no feature is left out. If necessary, continue generating additional files to include all requirements
                        """,
    output_file="features6.csv",
    agent=feature_generation_agent,
    context=[requirements]
)

# Creating Task for Scenario Generation from Features
scenario_generation_task = Task(
    description=f"""For each feature identified in the previous task, generate all possible scenarios, including positive, negative, boundary, and edge cases  using the provided Software Requirement Specifications(SRS) .
                    """,
    expected_output="""Format your response as if you were describing the contents of an Excel file where each scenario is described clearly for each feature:
                        Feature ID | Scenario ID | Scenario Description

                        FTE-001 | SCENARIO-001 | <scenario description>
                        FTE-002 | SCENARIO-001 | <scenario description>
                           .
                           .
                           .
                    IMPORTANT: Your response should be a valid CSV file (.csv) with pipe (|) delimiter.
                               Your response must consist only of the table structure.
                               Do not add any additional text, explanations, or summary lines before or after the table.
                               The response should start with the header row and end with the last data row of the table.
                               Ensure comprehensive coverage of scenarios for each feature.Ensure no scenario is left out. If necessary, continue generating all scenarios
                    """,
    output_file="scenarios6.csv",
    agent=scenario_generation_agent,
    context=[feature_generation_task, requirements]
)

testcases = Task(
    description=f"""Based on the provided system requirements  using the provided Software Requirement Specifications(SRS) , generate comprehensive Gherkin test cases in a tabular format suitable for an Excel file with columns.
                    """,
    expected_output="""Format your response as if you were describing the contents of an Excel file:

                        Test Case ID | Feature | Scenario | Steps | Requirement ID

                        TEST-001     | <feature_name> | <scenario_name> | Given <precondition> When <action> And <action> But <action> Then <expected_result> And <expected_result>| <requirement_id>

                        TEST-002     | <feature_name> | <scenario_name> | Given <precondition> When <action> And <action> But <action> Then <expected_result> And <expected_result>| <requirement_id>

                            .
                            .
                            .
                    IMPORTANT: Your response should be a valid Excel file (.xlsx).
                               Your response must consist only of the table structure.
                               Do not add any additional text, explanations, or summary lines before or after the table.
                               The response should start with the header row and end with the last data row of the table.
                               Ensure all features and scenarios are covered.
                               Ensure no testcase is left out. If necessary, Continue generating all testcases
                    """,
    output_file="testcases6.csv",
    agent=testcase_agent,
    context=[feature_generation_task,scenario_generation_task, requirements]
)

# Review by QA agent
quality_assurance_task = Task(
    description=f"""You are a Quality Assurance expert tasked with reviewing the test cases generated in the previous step.
                  Identify any missing test cases, edge cases, or boundary conditions that have not been covered by the previous set of test cases  using the provided Software Requirement Specifications(SRS) .
                  Review the test cases and generate the necessary additions, if any.
                  """,
    expected_output="""Provide a table with the missing test cases identified, if any.
                      The table should contain:

                      Test Case ID | Feature | Scenario | Steps | Requirement ID

                      TEST-004     | <feature_name> | <scenario_name> | Given <precondition> When <action> And <action> But <action> Then <expected_result> And <expected_result>| <requirement_id>

                      TEST-009     | <feature_name> | <scenario_name> | Given <precondition> When <action> And <action> But <action> Then <expected_result> And <expected_result>| <requirement_id>

                          .
                          .
                          .
                      """,
    output_file="missing_test_cases6.csv",
    agent=quality_assurance_agent,
    context=[testcases,requirements]
)


# Creating a task to use the Behave Script Generator agent
behave_script_task = Task(
    description="""Convert the provided features and scenarios into Behave feature files and Python step definitions.
                  Generate the following output for each feature:

                  1. A feature file (.feature) containing all scenarios for the feature.
                  2. A Python step definition file (.py) implementing the steps for the scenarios.

                  Ensure the generated files adhere to the following structure:

                  **Feature File Format (.feature):**
                  Feature: <Feature Description>

                    Scenario: <Scenario Description>
                        Given <precondition>
                        When <action>
                        And <action>
                        But <action
                        Then <expected result>
                        And <expected result>

                  **Step Definition File Format (.py):**
                  # Imports
                  from behave import given, when, then

                  # Step Definitions
                  @given('<precondition>')
                  def step_given(context):
                      pass

                  @when('<action>')
                  def step_when(context):
                      pass

                  @then('<expected result>')
                  def step_then(context):
                      pass

                **How Gherkin is converted to Behave:**

                The conversion process involves the following steps:

                1. **Feature Files:** The Gherkin feature file (`.feature`) remains largely unchanged. It serves as the blueprint for the tests.
                2. **Step Definitions:** Each step in the feature file (Given, When, Then) is mapped to a Python function in the step definition file (`.py`).
                3. **Decorators:** Behave uses decorators (`@given`, `@when`, `@then`) to link the Gherkin steps to their corresponding Python functions.
                4. **Context:** The `context` object is passed to each step function. It's used to share data between steps and store the state of the test.
                5. **Assertions:** Assertions are used in the step definition functions to verify the expected outcomes of the test.

                ```
                **Example Input Feature:**
                        Feature: User Login

                          Scenario: Successful Login with Valid Credentials
                              Given the user is on the login page
                              When the user enters valid credentials
                              Then the user is redirected to the dashboard

                   ** Example Output Files:**

                    **user_login.feature**
                    Feature: User Login

                      Scenario: Successful Login with Valid Credentials
                          Given the user is on the login page
                          When the user enters valid credentials
                          Then the user is redirected to the dashboard

                    **user_login.py**
                    from behave import given, when, then

                    @given("the user is on the login page")
                    def step_impl(context):
                        context.page = "login page"

                    @when("the user enters valid credentials")
                    def step_impl(context):
                        context.credentials = "valid"

                    @then("the user is redirected to the dashboard")
                    def step_impl(context):
                        assert context.credentials == "valid"
                        context.page = "dashboard"
                        assert context.page == "dashboard"
                ```

                    Explanation:
                    - **Feature File:**
                        - Written in Gherkin syntax.
                        - Defines the feature, scenario, and steps (Given, When, Then).
                    - **Step Definition File:**
                        - Implements the logic for each step using Python functions.
                        - Decorators (`@given`, `@when`, `@then`) link the Gherkin steps to Python functions.
                        - The `context` object shares data between steps and validates outcomes using assertions.

                  **IMPORTANT:** Ensure comprehensive coverage of all scenarios and proper mapping between feature files and step definitions.
                                 Ensure no feature is left out. If necessary, continue generating additional files to include all feature files""",
    expected_output="""Provide a json format containing all the generated feature files and step definition scripts.
                      Each feature should have a corresponding feature file (.feature) and step definition file (.py).
                      The file should be well-organized into subfolders if there are multiple features.
                        **IMPORTANT**: Ensure no feature is left out. If necessary, continue generating additional files to include all features
                            {
                                "adaptive_cruise_control": (
                                    "Feature: Adaptive Cruise Control
                                        Scenario: Maintain speed in free road
                                            Given the vehicle is moving at 60 mph
                                            When there are no vehicles ahead
                                            Then the vehicle should maintain its speed
                                    ",
                                   "from behave import given, when, then

                                    @given('the vehicle is moving at {speed} mph')
                                    def given_vehicle_speed(context, speed):
                                        context.speed = int(speed)

                                    @when('there are no vehicles ahead')
                                    def when_no_vehicles_ahead(context):
                                        context.road_clear = True

                                    @then('the vehicle should maintain its speed')
                                    def then_maintain_speed(context):
                                        assert context.road_clear and context.speed == 60
                                    "
                                    ),
                                "lane_keeping_assist": (
                                    "Feature: Lane Keeping Assist
                                        Scenario: Keep the vehicle centered in lane
                                            Given the vehicle is moving within a lane
                                            When the vehicle drifts slightly
                                            Then the system should apply corrective steering
                                    ",
                                       "from behave import given, when, then

                                        @given('the vehicle is moving within a lane')
                                        def given_vehicle_in_lane(context):
                                            context.in_lane = True

                                        @when('the vehicle drifts slightly')
                                        def when_vehicle_drifts(context):
                                            context.drifts = True

                                        @then('the system should apply corrective steering')
                                        def then_correct_steering(context):
                                            assert context.in_lane and context.drifts
                                    "
                                )
                            }
                      """,
    output_file="behave_scripts6.json",
    agent=behave_script_agent,
    context=[testcases, quality_assurance_task]
)

# Make sure agents are instantiated properly and avoid any conflict with the configuration.
agents = [requirement_agent, feature_generation_agent, scenario_generation_agent, testcase_agent, quality_assurance_agent, behave_script_agent]

tasks = [requirements, feature_generation_task, scenario_generation_task, testcases, quality_assurance_task, behave_script_task]

# Initialize the Crew object
crew = Crew(
    agents=agents,
    tasks=tasks,
    process=Process.sequential,
    full_output=True,
    verbose=True
)

# Kickoff the process
responses = crew.kickoff()


def csv_to_excel(csv_file, excel_file,delimiter):
    """Converts a CSV file to an Excel file using pandas.

    Args:
        csv_file (str): Path to the CSV file.
        excel_file (str): Path to the output Excel file.
    """

    try:
        # Read the CSV file into a pandas DataFrame
        df = pd.read_csv(csv_file,delimiter=delimiter)
        df = df.iloc[:, 1:-1]
        # Write the DataFrame to an Excel file
        df.to_excel(excel_file, index=False)

        print("Conversion successful!")
    except Exception as e:
        print(f"Error occurred during conversion: {e}")


csv_to_excel("requirements6.csv", "requirements6.xlsx","|")
csv_to_excel("features6.csv", "features6.xlsx", "|")
csv_to_excel("scenarios6.csv", "scenarios6.xlsx", "|")
csv_to_excel("testcases6.csv", "testcases6.xlsx","|")
csv_to_excel("missing_test_cases6.csv", "missing_test_cases6.xlsx","|")



```

