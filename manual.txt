The complexity of Advanced Driver Assistance Systems (ADAS)/Autonomous Driving (AD) necessitates efficient system testing and validation processes. The verification and testing of ADAS and autonomous driving functions relies heavily on scenario-based testing (SBT), where diverse real-world traffic scenarios are simulated to ensure system safety and reliability. Behavior-Driven Development (BDD) offers a structured, natural language approach to defining test scenarios, but its manual implementation remains time-consuming and error-prone. In this paper, we propose a Large Language Model (LLM)-based Multi-Agent System (MAS) approach to automate BDD in scenario-based verification of ADAS. Our framework leverages LLMs and MAS to generate, translate, and refine BDD scenarios into executable test cases. The system enhances efficiency by automating test scenario generation, reducing human effort, and improving adaptability to complex verification tasks. We evaluate our approach using real-world ADAS scenarios in simulation, demonstrating significant improvements in automation, scalability, and verification accuracy compared to traditional methods. This research highlights the potential of LLM-driven multi-agent frameworks in streamlining scenario-based testing for autonomous systems, paving the way for more robust and intelligent verification pipelines in the automotive industry.
